stages:
  # --- STAGE 0: COMMON BASE (Deduplication) ---
  # Rimuove i duplicati mantenendo la struttura train/val/test
  deduplicate:
    cmd: python src/preprocessing/deduplicate.py --input data/raw --output data/deduplicated
    deps:
      - data/raw
      - src/preprocessing/deduplicate.py
    outs:
      - data/deduplicated

  # =======================================================
  # PIPELINE 1: BASELINE (Raw Clean Data)
  # =======================================================
  train_p1:
    foreach: [effnet_v2_s, convnext]
    do:
      # Uso: Train pulito, Valid pulito
      cmd: python -m src.training.train --model ${item} --train data/deduplicated/train --val data/deduplicated/valid --out models/p1/${item} --batch 16 --epochs 4
      deps:
        - data/deduplicated/train
        - data/deduplicated/valid
        - src/training/train.py
      outs:
        - models/p1/${item}/model.pth
        - models/p1/${item}/loss_plot.png
        - models/p1/${item}/history.json

  evaluate_p1:
    foreach: [effnet_v2_s, convnext]
    do:
      # Valutazione finale su Test Set pulito
      cmd: python -m src.training.evaluate --model ${item} --weights models/p1/${item}/model.pth --data data/deduplicated/test --out models/p1/${item}
      deps:
        - data/deduplicated/test
        - src/training/evaluate.py
        - models/p1/${item}/model.pth
      metrics:
        - models/p1/${item}/metrics.json:
            cache: false
      plots:
        - models/p1/${item}/confusion_matrix.png:
            cache: false

  # =======================================================
  # PIPELINE 2: DATA AUGMENTATION
  # =======================================================
  augment_p2:
    # Aumentiamo SOLO il train set
    cmd: python src/preprocessing/augment.py --input data/deduplicated/train --output data/augmented/train
    deps:
      - data/deduplicated/train
      - src/preprocessing/augment.py
    outs:
      - data/augmented/train

  train_p2:
    foreach: [effnet_v2_s, convnext]
    do:
      # Uso: Train AUMENTATO, ma Valid PULITO (fondamentale per controllo reale)
      cmd: python -m src.training.train --model ${item} --train data/augmented/train --val data/deduplicated/valid --out models/p2/${item} --batch 16 --epochs 4
      deps:
        - data/augmented/train
        - data/deduplicated/valid
        - src/training/train.py
      outs:
        - models/p2/${item}/model.pth
        - models/p2/${item}/loss_plot.png
        - models/p2/${item}/history.json

  evaluate_p2:
    foreach: [effnet_v2_s, convnext]
    do:
      cmd: python -m src.training.evaluate --model ${item} --weights models/p2/${item}/model.pth --data data/deduplicated/test --out models/p2/${item}
      deps:
        - data/deduplicated/test
        - src/training/evaluate.py
        - models/p2/${item}/model.pth
      metrics:
        - models/p2/${item}/metrics.json:
            cache: false
      plots:
        - models/p2/${item}/confusion_matrix.png:
            cache: false

# =======================================================
  # PIPELINE 3: BALANCING (Two-Phase Learning)
  # =======================================================

  # 1. Creazione Dataset Bilanciato (Undersampling)
  balance_p3:
    # Input: Dataset deduplicato | Output: Dataset sottocampionato (data/balanced)
    cmd: python -m src.preprocessing.balance --input data/deduplicated/train --output data/balanced/train
    deps:
      - data/deduplicated/train
      - src/preprocessing/balance.py
    outs:
      - data/balanced/train

  # 2. FASE 1: Training su Dataset BILANCIATO
  # Obiettivo: Imparare le feature senza bias verso le classi maggioritarie.
  # Usiamo poche epoche (es. 10) perché il dataset è molto più piccolo.
  train_p3_phase1:
    foreach: [effnet_v2_s, convnext]
    do:
      cmd: >
        python -m src.training.train 
        --model ${item} 
        --train data/balanced/train 
        --val data/deduplicated/valid 
        --out models/p3_phase1/${item} 
        --batch 16 
        --epochs 4
      deps:
        - data/balanced/train
        - data/deduplicated/valid
        - src/training/train.py
      outs:
        - models/p3_phase1/${item}/model.pth
        - models/p3_phase1/${item}/loss_plot.png

  # 3. FASE 2: Fine-Tuning su Dataset ORIGINALE (Sbilanciato)
  # Obiettivo: Adattare il modello alla distribuzione reale dei dati.
  # Carichiamo i pesi della Fase 1 (--weights) e addestriamo sul dataset completo.
  train_p3_phase2:
    foreach: [effnet_v2_s, convnext]
    do:
      cmd: >
        python -m src.training.train 
        --model ${item} 
        --train data/deduplicated/train 
        --val data/deduplicated/valid 
        --out models/p3_phase2/${item} 
        --weights models/p3_phase1/${item}/model.pth
        --batch 16 
        --epochs 4
      deps:
        - data/deduplicated/train
        - data/deduplicated/valid
        - src/training/train.py
        - models/p3_phase1/${item}/model.pth
      outs:
        - models/p3_phase2/${item}/model.pth
        - models/p3_phase2/${item}/loss_plot.png
        - models/p3_phase2/${item}/history.json

  # 4. Valutazione Finale (Usando il modello finale della Fase 2)
  evaluate_p3:
    foreach: [effnet_v2_s, convnext]
    do:
      cmd: python -m src.training.evaluate --model ${item} --weights models/p3_phase2/${item}/model.pth --data data/deduplicated/test --out models/p3_phase2/${item}
      deps:
        - data/deduplicated/test
        - src/training/evaluate.py
        - models/p3_phase2/${item}/model.pth
      metrics:
        - models/p3_phase2/${item}/metrics.json:
            cache: false
      plots:
        - models/p3_phase2/${item}/confusion_matrix.png:
            cache: false
        - models/p3_phase2/${item}/classification_report.png:
            cache: false

  # =======================================================
  # PIPELINE 4: HYBRID (Augment -> Balance)
  # =======================================================
  augment_p4:
    # Step 1: Augmentation del train originale
    cmd: python src/preprocessing/augment.py --input data/deduplicated/train --output data/tmp/p4_aug
    deps:
      - data/deduplicated/train
      - src/preprocessing/augment.py
    outs:
      - data/tmp/p4_aug

  balance_p4:
    # Step 2: Balancing sul dataset aumentato
    cmd: python src/preprocessing/balance.py --input data/tmp/p4_aug --output data/hybrid/train
    deps:
      - data/tmp/p4_aug
      - src/preprocessing/balance.py
    outs:
      - data/hybrid/train

  train_p4:
    foreach: [effnet_v2_s, convnext]
    do:
      # Uso: Train IBRIDO, Valid PULITO
      cmd: python -m src.train --model ${item} --train data/hybrid/train --val data/deduplicated/valid --out models/p4/${item} --batch 16 --epochs 20
      deps:
        - data/hybrid/train
        - data/deduplicated/valid
        - src/train.py
      outs:
        - models/p4/${item}/model.pth
        - models/p4/${item}/loss_plot.png
        - models/p4/${item}/history.json

  evaluate_p4:
    foreach: [effnet_v2_s, convnext]
    do:
      cmd: python -m src.evaluate --model ${item} --weights models/p4/${item}/model.pth --data data/deduplicated/test --out models/p4/${item}
      deps:
        - data/deduplicated/test
        - src/evaluate.py
        - models/p4/${item}/model.pth
      metrics:
        - models/p4/${item}/metrics.json:
            cache: false
      plots:
        - models/p4/${item}/confusion_matrix.png:
            cache: false